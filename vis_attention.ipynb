{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualized Transformer Attention with DINO__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import timm\n",
    "import dino\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ViT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\bjohnson/.cache\\torch\\hub\\facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "#model = timm.create_model('vit_small_patch16_224_dino',pretrained=True)\n",
    "model = torch.hub.load('facebookresearch/dino:main','dino_vits16')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"img1.jpg\")\n",
    "Tx = transforms.Resize((224,224))(image)\n",
    "Tx2 = transforms.ToTensor()(Tx).unsqueeze_(0)\n",
    "Tx3 = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(Tx2)\n",
    "Tx3.requires_grad = True\n",
    "\n",
    "print(Tx3.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 6, 197, 197])\n",
      "torch.Size([6, 196])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m th_attn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39minterpolate(th_attn\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), scale_factor\u001b[39m=\u001b[39mpatch_size, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     33\u001b[0m attentions \u001b[39m=\u001b[39m attentions\u001b[39m.\u001b[39mreshape(nh, w_featmap\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m, h_featmap\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m attentions \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49minterpolate(attentions\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), scale_factor\u001b[39m=\u001b[39;49mpatch_size, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnearest\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m     35\u001b[0m attentions_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(attentions, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "attentions = model.get_last_selfattention(Tx3)\n",
    "\n",
    "print(Tx3.shape)\n",
    "print(attentions.shape)\n",
    "\n",
    "nh = attentions.shape[1]\n",
    "attentions = attentions[0, :, 0, 1:].reshape(nh,-1)\n",
    "print(attentions.shape)\n",
    "\n",
    "val, idx = torch.sort(attentions)\n",
    "val /= torch.sum(val, dim=1, keepdim=True)\n",
    "cumval = torch.cumsum(val, dim=1)\n",
    "\n",
    "patch_size = 8\n",
    "\n",
    "w_featmap = Tx3.shape[-2] // patch_size\n",
    "h_featmap = Tx3.shape[-1] // patch_size\n",
    "\n",
    "threshold = 0.6 # We visualize masks obtained by thresholding the self-attention maps to keep xx% of the mass.\n",
    "th_attn = cumval > (1 - threshold)\n",
    "idx2 = torch.argsort(idx)\n",
    "for head in range(nh):\n",
    "    th_attn[head] = th_attn[head][idx2[head]]\n",
    "    \n",
    "th_attn = th_attn.reshape(nh, w_featmap//2, h_featmap//2).float()\n",
    "\n",
    "# interpolate\n",
    "th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "attentions = attentions.reshape(nh, w_featmap//2, h_featmap//2)\n",
    "attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\n",
    "attentions_mean = np.mean(attentions, axis=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
