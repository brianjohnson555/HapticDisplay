{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualized Transformer Attention with DINO__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.animation as animation\n",
    "from time import sleep\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/bjons/.cache/torch/hub/facebookresearch_dino_main\n",
      "Using cache found in /home/bjons/.cache/torch/hub/facebookresearch_dino_main\n",
      "Using cache found in /home/bjons/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "/home/bjons/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/bjons/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n",
      "Using cache found in /home/bjons/.cache/torch/hub/intel-isl_MiDaS_master\n",
      "/home/bjons/.local/lib/python3.10/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n",
      "Using cache found in /home/bjons/.cache/torch/hub/intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "#model = timm.create_model('vit_small_patch16_224_dino',pretrained=True)\n",
    "dino8 = torch.hub.load('facebookresearch/dino:main','dino_vits8')\n",
    "dino8.to(device)\n",
    "dino8.eval()\n",
    "dino16 = torch.hub.load('facebookresearch/dino:main','dino_vits16')\n",
    "dino16.to(device)\n",
    "dino16.eval()\n",
    "# model = torch.hub.load('facebookresearch/dinov2','dinov2_vits14')\n",
    "# midas = torch.hub.load('intel-isl/MiDaS','DPT_Hybrid')\n",
    "midas_s = torch.hub.load('intel-isl/MiDaS','MiDaS_small')\n",
    "midas_s.to(device)\n",
    "midas_s.eval()\n",
    "midas_h = torch.hub.load('intel-isl/MiDaS','DPT_Hybrid')\n",
    "midas_h.to(device)\n",
    "midas_h.eval()\n",
    "# midas = torch.hub.load('intel-isl/MiDaS', 'custom', path='../utilites/dpt_beit_large_512.pt', force_reload=True)\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "transform = midas_transforms.small_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get last self attention function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_self_attention(self, x, masks=None):\n",
    "    if isinstance(x, list):\n",
    "        return self.forward_features_list(x, masks)\n",
    "        \n",
    "    x = self.prepare_tokens_with_masks(x, masks)\n",
    "    \n",
    "    # Run through model, at the last block just return the attention.\n",
    "    for i, blk in enumerate(self.blocks):\n",
    "        if i < len(self.blocks) - 1:\n",
    "            x = blk(x)\n",
    "        else: \n",
    "            return blk(x, return_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up frame capture and initialize frame lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup frame capture\n",
    "video = 'boat'\n",
    "cap = cv2.VideoCapture(\"video_\" + video + \".mp4\") #use video\n",
    "#cap = cv2.VideoCapture(0) #stream from webcam\n",
    "previous_frame = None\n",
    "\n",
    "frames_list = []\n",
    "attentions_list = []\n",
    "depth_list = []\n",
    "combined_list = []\n",
    "Hasel_list = []\n",
    "imsize = [5, 10, 15]\n",
    "max_frames = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame loop and model evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x56336d23a080] mmco: unref short failure\n",
      "[h264 @ 0x56336d23a080] mmco: unref short failure\n"
     ]
    }
   ],
   "source": [
    "for framenum in range(max_frames):\n",
    "    attentions_mean = [[], [], []]\n",
    "    depth = [[], [], []]\n",
    "    combined = [[], [], []]\n",
    "    Hasel = [[], [], []]\n",
    "    # Load frame\n",
    "    for frameskip in range(5):\n",
    "        ret= cap.grab()\n",
    "        if ret is False:\n",
    "            break\n",
    "        ret, img = cap.retrieve()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    frame = img\n",
    "    # img = cv2.addWeighted(img,1.5,img,0,1)\n",
    "    \n",
    "    ### Compute attention ###    \n",
    "    image = Image.fromarray(img)\n",
    "    input_batch = transform(img).to(device)\n",
    "    for ii in range(0,len(imsize)):\n",
    "        Tx = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(transforms.ToTensor()(transforms.Resize((imsize[ii]*9,imsize[ii]*16))(image)).unsqueeze_(0))\n",
    "        Tx.requires_grad = True\n",
    "\n",
    "        attentions = dino8.get_last_selfattention(Tx)\n",
    "    \n",
    "        nh = attentions.shape[1]\n",
    "        attentions = attentions[0, :, 0, 1:].reshape(nh,-1)\n",
    "        patch_size = 4\n",
    "        w_featmap = Tx.shape[-2] // patch_size\n",
    "        h_featmap = Tx.shape[-1] // patch_size\n",
    "\n",
    "        attentions = attentions.reshape(nh, w_featmap//2, h_featmap//2)\n",
    "        attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=1, mode=\"nearest\")[0].detach().numpy()\n",
    "        attentions_mean[ii] = np.mean(attentions, axis=0)\n",
    "\n",
    "    ### Compute depth ###\n",
    "    # start_time = time.time()\n",
    "    # print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = midas_s(input_batch)\n",
    "            prediction = torch.nn.functional.interpolate(\n",
    "                prediction.unsqueeze(1),\n",
    "                size=(imsize[ii]*9, imsize[ii]*16),\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            ).squeeze()\n",
    "        depth[ii] = prediction.numpy()\n",
    "    \n",
    "        new_size = attentions_mean[ii].shape\n",
    "\n",
    "        depth_re = cv2.resize(depth[ii], dsize=(new_size[1], new_size[0]), interpolation=cv2.INTER_CUBIC)\n",
    "        depth_nm = cv2.normalize(depth_re, None, 0, 1, norm_type=cv2.NORM_MINMAX, dtype = cv2.CV_64F)\n",
    "        attentions_nm = cv2.normalize(attentions_mean[ii], None, 0, 1, norm_type=cv2.NORM_MINMAX, dtype = cv2.CV_64F)\n",
    "        combined[ii] = depth_nm * attentions_nm\n",
    "        combined[ii] = (combined[ii] > 0.1) * combined[ii]\n",
    "        Hasel[ii] = cv2.resize(combined[ii], dsize=(10, 6), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    \n",
    "    ### Update ###\n",
    "    attentions_list.append(attentions_mean)\n",
    "    depth_list.append(depth)\n",
    "    frames_list.append([frame, frame, frame])\n",
    "    combined_list.append(combined)\n",
    "    Hasel_list.append(Hasel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process frames for animations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [frames_list, attentions_list, depth_list, combined_list, Hasel_list]\n",
    "name_list = [\"frames\", \"attention\", \"depth\", \"combined\", \"HASEL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting 0 list data\n",
      "Plotting 1 list data\n",
      "Plotting 2 list data\n",
      "Plotting 3 list data\n",
      "Plotting 4 list data\n"
     ]
    }
   ],
   "source": [
    "for datanum in range(len(data_list)):\n",
    "    print(\"Plotting %i list data\" % (datanum))\n",
    "    for sizenum in range(len(imsize)):\n",
    "        ims = []\n",
    "        figure = plt.figure()\n",
    "        for framenum in range(len(frames_list)):\n",
    "            im = plt.imshow(data_list[datanum][framenum][sizenum], animated=True)\n",
    "            ims.append([im])\n",
    "        ani = animation.ArtistAnimation(figure, ims, blit=True, repeat=False)\n",
    "        filename = \"OutputVideos/animation_\" + video + \"_\" + name_list[datanum] + \"_\" + str(imsize[sizenum]) + \".mp4\"\n",
    "        ani.save(filename, writer = \"ffmpeg\", bitrate=1000, fps=5)\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
