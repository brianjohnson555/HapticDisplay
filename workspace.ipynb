{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\bjohnson/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "Using cache found in C:\\Users\\bjohnson/.cache\\torch\\hub\\intel-isl_MiDaS_master\n",
      "Using cache found in C:\\Users\\bjohnson/.cache\\torch\\hub\\facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This file is a test playground for new scripts and functions.\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "###### USER SETTINGS ######\n",
    "VIDEO = \"truck\" # pick video suffix from algo_input_videos/ folder\n",
    "RESOLUTION_ATT = 100 # resolution of get_attention for DINO model\n",
    "MODEL = 'hybrid' # MiDaS model type ('small', 'hybrid', 'large')\n",
    "THRESHOLD_VAL = 0.35 # threshold of attention+depth combination\n",
    "BIAS = 0.75 # bias towards attention for attention+depth combination\n",
    "SCALE = 4 # scaling of combined array (scale*[16, 9])\n",
    "DISPLAY_DIMS = (4,7) # HASEL haptic display dimensions, H x W (pixels)\n",
    "FRAME_SKIP = 5 # interval for how often to calculate algorithm (then interpolate between)\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "###### INITIALIZATIONS ######\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.animation as animation\n",
    "import visual_haptic_utils.algo_preprocessing as algo\n",
    "\n",
    "###### MAIN ######\n",
    "## Setting up torch device and model, video input\n",
    "device = torch.device(DEVICE)\n",
    "cap = cv2.VideoCapture(\"algo_input_videos/video_\" + VIDEO + \".mp4\") #use video\n",
    "model = algo.VisualHapticModel(device=device,\n",
    "                               resolution_attention=RESOLUTION_ATT,\n",
    "                               depth_model_type=MODEL,\n",
    "                               threshold_value=THRESHOLD_VAL,\n",
    "                               bias=BIAS,\n",
    "                               scaling_factor=SCALE,\n",
    "                               display_dim=DISPLAY_DIMS)\n",
    "\n",
    "frame_num = 1\n",
    "output_list = []\n",
    "depth_list = []\n",
    "attention_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = algo.grab_video_frame(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_frame = model.get_attention(frame)\n",
    "depth = model.get_depth(frame)\n",
    "depth_frame = model.remove_gradient(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_frame = model.get_combined(depth_frame, attention_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_frame = model.get_threshold(combined_frame)\n",
    "output = model.get_downsample(threshold_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.run(frame) # run the visual-haptic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            if output_list:#skip first iteration\n",
    "                last_output = output_list[-1]\n",
    "                last_depth = depth_list[-1]\n",
    "                last_att = attention_list[-1]\n",
    "                for frame in range(1, FRAME_SKIP):\n",
    "                    interp_vec = np.zeros(DISPLAY_DIMS)\n",
    "                    interp_vec2 = np.zeros(model.depth_frame.shape)\n",
    "                    interp_vec3 = np.zeros(model.attention_frame.shape)\n",
    "                    for row in range(DISPLAY_DIMS[0]):\n",
    "                        for col in range(DISPLAY_DIMS[1]):\n",
    "                            interp_vec[row, col] = np.linspace(last_output[row, col], output[row, col], FRAME_SKIP+1)[frame]\n",
    "                    output_list.append(interp_vec)\n",
    "\n",
    "            output_list.append(output)\n",
    "            print(len(output_list))\n",
    "\n",
    "        frame_num += 1\n",
    "        print(frame_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
