#!/usr/bin/env python

"""This demo script simultaneously displays the koi fish video will writing haptic output
to the haptic display via USB. Both the video and haptic output will loop continuously
until the user presses the 'B' keyboard key.

To run, it requires both the input video and the output.txt file generated by the
visual-haptic algorithm preprocessing code."""

###### USER SETTINGS ######
SERIAL_ACTIVE = True # if False, just runs the algorithm without sending to HV switches
VIEW_INTENSITY = False # if True, opens a video showing the depth/intensity map
SAVE_VIDEO = False # if True, saves a video file of both the camera view and the intensity map
COM_A = "COM9" # port for MINI switches 1-10
COM_B = "COM15" # port for MINI switches 11-20
COM_C = "COM16" # port for MINI swiches 21-28

###### INITIALIZATIONS ######
import cv2
import time
import serial
import numpy as np
import utils.algo_functions as algo_functions # my custom file
import utils.algo_gesture as algo_gesture # my custom file
import matplotlib.pyplot as plt

###### MAIN ######

# Set up USBWriter:
serial_ports = [COM_A, COM_B, COM_C]
USB_writer = algo_functions.USBWriter(serial_ports, serial_active=SERIAL_ACTIVE)
if SAVE_VIDEO:
    outlist_image = []
    outlist_haptic = []

# Enable HV!!!
USB_writer.HV_enable()

# initialize camera and gesture model:
cap = cv2.VideoCapture(0) #stream from webcam
gesture = algo_gesture.Gesture() # keep track of each recurrance of gestures
recognizer = algo_gesture.Recognizer(gesture)

with recognizer.recognizer as gesture_recognizer: #GestureRecognizer type needs "with...as" in order to run properly (enter()/exit())
    while True:
        ret, frame = cap.read()
        frame_timestamp_ms = int(np.floor(time.time() * 1000))
        gesture.get_latest_gesture(gesture_recognizer, frame_timestamp_ms, frame)

        intensity_array = gesture.output_latest
        haptic_output = algo_functions.map_intensity(intensity_array) # map from algo intensity to duty cycle/period
        USB_writer.write_to_USB(haptic_output)

        frame_annotated = cv2.putText(frame, 
                               str(gesture.gesture_active), 
                               org=(20, 200), 
                               fontFace=cv2.FONT_HERSHEY_SIMPLEX, 
                               fontScale=2, 
                               color=(255,0,0), 
                               thickness=2) # add current gesture annotation to image
        cv2.namedWindow('Video',cv2.WINDOW_KEEPRATIO)
        cv2.resizeWindow('Video', 4*192, 4*108)
        cv2.imshow('Video',frame_annotated)

        if SAVE_VIDEO:
            outlist_image.append(frame_annotated)
            outlist_haptic.append(intensity_array)

        if(cv2.waitKey(10) & 0xFF == ord('b')):
            break # BREAK OUT OF LOOP WHEN "b" KEY IS PRESSED!

if SAVE_VIDEO:
    import matplotlib.animation as animation
    ims = []
    figure = plt.figure()
    for i in range(0,len(outlist_image)):
        im = plt.imshow(outlist_image[i], animated=True)
        ims.append([im])
    ani = animation.ArtistAnimation(figure, ims, blit=True, repeat=False)
    filename = "OutputVideos/gesture_output_camera.mp4"
    ani.save(filename, writer = "ffmpeg", bitrate=1000, fps=10)
    plt.close()

    ims = []
    figure = plt.figure()
    for i in range(0,len(outlist_haptic)):
        im = plt.imshow(outlist_haptic[i], animated=True)
        ims.append([im])
    ani = animation.ArtistAnimation(figure, ims, blit=True, repeat=False)
    filename = "OutputVideos/gesture_output_intensity.mp4"
    ani.save(filename, writer = "ffmpeg", bitrate=1000, fps=10)
    plt.close()
    
# Disable HV!!!
USB_writer.HV_disable()
time.sleep(0.5)
